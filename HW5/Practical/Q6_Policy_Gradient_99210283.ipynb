{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Policy_Gradient_normal.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "pycharm-d7999b95",
      "language": "python",
      "display_name": "PyCharm (deep_hw2_q5)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3OlGZrajdP6"
      },
      "source": [
        "from Q6_custom_environment_99210283 import custom\n",
        "from torch.distributions import Normal\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6QlMUUn-mzY"
      },
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self):\n",
        "        super (Policy, self).__init__()\n",
        "        self.FC = nn.Sequential (nn.Linear (2, 32), nn.Dropout(0.5), nn.ReLU(),\n",
        "                                  nn.Linear (32, 32), nn.Dropout(0.5), nn.ReLU(),\n",
        "                                  nn.Linear (32, 2))\n",
        "        \n",
        "    def forward(self, state):\n",
        "        return self.FC (torch.tensor([state[0], state[1]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "us-D2XGjmJKe",
        "scrolled": true,
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "optimizer = torch.optim.Adam(policy.parameters(), lr = learning_rate)\n",
        "learning_rate = 1e-3\n",
        "gamma = 0.9\n",
        "num_episodes= 10000\n",
        "num_max_step= 200\n",
        "policy = Policy()\n",
        "reward_history = []\n",
        "loss_history = []\n",
        "policy_history = []\n",
        "reward_episode = []\n",
        "env = custom()\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = env.Res()\n",
        "    for step in range(num_max_step):\n",
        "        mu, var = policy (state)\n",
        "        action_dist = Normal(mu, var)\n",
        "        action = action_dist.sample()\n",
        "        new_state, reward, done = env.Step(action)\n",
        "        state = torch.tensor (new_state)\n",
        "        reward_episode.append(reward)\n",
        "        policy_history.append(action_dist.log_prob(action)[None])\n",
        "\n",
        "    print(f'final state is {state}')\n",
        "    return_rewards=[]\n",
        "    \n",
        "    disc_reward = 0\n",
        "    for i,r in enumerate (reward_episode[::-1]):\n",
        "        \n",
        "        disc_reward = r + gamma * disc_reward\n",
        "        return_rewards.append (disc_reward)\n",
        "    return_rewards = torch.tensor (return_rewards).flip(dims = [-1])\n",
        "    \n",
        "\n",
        "    return_rewards = (return_rewards - return_rewards.mean()) / (return_rewards.std() + 1e-8)\n",
        "    \n",
        "    v = torch.cat (policy_history)\n",
        "    \n",
        "    loss = -(return_rewards * v).sum()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss = loss.item()\n",
        "    \n",
        "    loss_history.append(loss)\n",
        "    reward_history.append(np.sum(reward_episode)/step)\n",
        "\n",
        "    print('[Episode %d / %d] reward: %.3f' %\n",
        "                  (episode, num_episodes, np.sum(reward_episode)/step))\n",
        "\n",
        "    policy_history = []\n",
        "    reward_episode= []\n",
        "\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}